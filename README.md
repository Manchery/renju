# Renju

## Baseline: alpha-beta + eval (with 柱搜索)

博弈搜索的标准做法：

- MINIMAX
- 优化：alpha-beta
- 优化：alpha-beta + eval

这里我们的 baseline 就是：**普通的 alpha-beta 搜索 + 深度足够截断时返回局面估值**

注意到，以下不特殊说明，我们所有搜索策略中都包含所谓 **柱搜索**，即每步搜索只扩展最优的若干步。

因此我们实现了两种启发式估值：一种是对于整个局面的估值，一种是当前局面下某一步棋的估值。

## alpha-beta + eval + hash

运用 hash 也就是所谓的 **置换表** 或者 **换位表** 技术，目的是消除重复状态的搜索。

在这一小节里，假设每一步棋的搜索完成之后 **hash 表清空**

### 局面 hash 算法：zobrist

```C++
hash_t zobristValue[GRID_NUM][GRID_NUM][3];
hash_t whiteFirstValue, MinFirstValue;
```

`zobristValue` 中最后一维为 1,2 时非零，分别给这一个位置下了黑棋和白棋一个无符号 64位 整数的权值。

下面两个 FirstValue 同理，当局面是白棋下或者 MIN 下时给一个权值。（对于同一盘棋来说，这里是冗余的，因为：要么白是MIN，要么白不是MIN；也可以这么理解当前棋局数一下有多少棋子就能知道是白还是黑下；不过冗余无所谓了，只要相同局面的 hash 值相同就可以了(?)）

hash 值 `zobrist` 就是把所有权值异或在一起。

### 置换表与 alpha-beta 的结合

给每个 hash 节点一些成员：flag, val 分别表示节点类型和估值（MINIMAX值）。

flag 分为 EXACT, LOWER, UPPER 分别表示估值搜索出来就是 val，至少，至多是 val（后两种情况是因为没有搜完被剪枝了）

注意到我们假定每次搜索完都清空了 hash 表，那么如果在一次搜索过程中碰到了重复节点，那么他们即将要搜的depth 是一定相同的（在下面 ID Search 里就不能保证），分情况讨论：

- flag 是 EXACT，因为搜索深度一样，结果肯定是一样的，直接返回
- flag 是 LOWER，这只可能发生在 MIN 决策时，而 alpha 在一次搜索中是不降的，那么之前被剪枝了，现在同样会被剪枝，直接返回。
- flag 是 UPPER，只可能发生在 MAX 决策时，直接返回，理由同上。

至此，我们的 hash 表中**不需要存储** 搜索深度、最优行动这两个信息，因为在这里，hash 表发挥的唯一作用是将搜索树中的相同节点合并为搜索图。

## alpha-beta + eval + hash + ID

现在引入迭代加深搜索。以下我们假定，每次搜索之后不清空 hash 表。且在 hash 表中新增成员 depth 和 move，分别表示搜索深度和**搜索到**的最优行动（注意可能不是该搜索深度下的最优行动，因为可能搜到一半被剪枝）。

现在如果遇到一个已经被搜索到的局面：

- 如果 depth == 当前所需的搜索深度
  - 如果是 flag 是 EXACT，直接返回没问题
  - 如果是 flag 是 LOWER，如果 val < 当前的 alpha，那么就可以直接返回，否则还要重新搜
  - 如果是 flag 是 UPPER，如果 val > 当前的 beta，那么就可以直接返回，否则还要重新搜
- 除开以上直接返回的情况，其余情况，hash 节点中存储的 move，都可以作为**行棋排序**的指导：一种简单的方式是把这个 move 放在第一个扩展。

### hash 表的清理

注意到由于我们不清空 hash 表，会导致内存占用越来越大：走完一步棋之后，**可能**其他分支中的局面再也不会被搜索到了。

一种清理的方法是在 hash 节点中记录上一次更新（访问）的 timeStamp。

每个一段时间，扫描 hash 表，如果很久没被访问过（设定一个阈值），那么就从表中删除它。

### 一种可能的优化(?)

注意到在上一步棋的搜索过程中可能就已经搜索到了当前的局面。

因此我们这一次迭代加深可以不从深度 1 开始搜，而是从 hash 表中已经存储了的深度开始搜？

（这是存疑的，还需进一步思考剪枝的影响，因为剪枝的存在，之前的更浅的搜索可能是不完整的）

~~分析不出来，看实验结果吧~~

## alpha-beta + eval + hash + ID + window

所谓的 **期望窗口** 技术是对 ID 的改进，只要每次搜索不从 $[\alpha, \beta ]$ 为 $[-\infty, \infty]$ 开始，而是 从上一深度的搜索返回的估值的一个邻域（比如正负一个手动设定的值），可能可以剪更多的枝，搜的更深。

这一做法的依据是，上一深度的搜索结果，不会与这一深度的搜索结果相差太多。

注意如果超参数设定不好，这可能反而是有害的

**观察实际迭代加深搜索过程中不同深度搜出来的估值，变化过大，该优化可能无法实施，弃置**